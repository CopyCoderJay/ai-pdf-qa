{"ast":null,"code":"import * as pdfjsLib from 'pdfjs-dist';\n\n// Set up PDF.js worker\npdfjsLib.GlobalWorkerOptions.workerSrc = `//cdnjs.cloudflare.com/ajax/libs/pdf.js/${pdfjsLib.version}/pdf.worker.min.js`;\nclass PDFProcessor {\n  constructor() {\n    this.chunkSize = 1000; // Increased from 500 to 1000 words\n    this.overlap = 100; // Increased overlap for better context\n  }\n  async processPDF(file) {\n    try {\n      // Convert file to ArrayBuffer\n      const arrayBuffer = await file.arrayBuffer();\n\n      // Load PDF document\n      const pdf = await pdfjsLib.getDocument({\n        data: arrayBuffer\n      }).promise;\n      const textChunks = [];\n      let totalTextLength = 0;\n      let contentPages = 0;\n      let skippedPages = 0;\n      console.log(`Processing PDF with ${pdf.numPages} pages...`);\n\n      // Extract text from each page, but be smarter about skipping headers\n      for (let pageNum = 1; pageNum <= pdf.numPages; pageNum++) {\n        const page = await pdf.getPage(pageNum);\n        const textContent = await page.getTextContent();\n\n        // Combine text items\n        const pageText = textContent.items.map(item => item.str).join(' ');\n        const trimmedText = pageText.trim();\n        const textLength = trimmedText.length;\n\n        // Skip pages that are likely headers, copyright, or table of contents\n        if (this.isHeaderPage(trimmedText, pageNum)) {\n          console.log(`Skipping page ${pageNum} (header/toc: ${textLength} chars)`);\n          skippedPages++;\n          continue;\n        }\n\n        // Only start collecting content after we've seen some substantial pages\n        if (contentPages < 2 && textLength < 500) {\n          console.log(`Skipping page ${pageNum} (too short for content: ${textLength} chars)`);\n          skippedPages++;\n          continue;\n        }\n        totalTextLength += textLength;\n        contentPages++;\n        console.log(`Page ${pageNum}: ${textLength} characters - ACCEPTED`);\n\n        // Split text into chunks\n        const chunks = this.splitTextIntoChunks(trimmedText, pageNum);\n        textChunks.push(...chunks);\n\n        // Limit to first 30 content pages to avoid overwhelming the API\n        if (contentPages >= 30) {\n          console.log(`Reached limit of 30 content pages, stopping extraction`);\n          break;\n        }\n      }\n      console.log(`Skipped ${skippedPages} header/toc pages`);\n      console.log(`Extracted ${contentPages} content pages with ${textChunks.length} chunks`);\n      console.log(`Total text length: ${totalTextLength} characters`);\n      return {\n        totalPages: pdf.numPages,\n        contentPages: contentPages,\n        skippedPages: skippedPages,\n        totalChunks: textChunks.length,\n        textChunks: textChunks,\n        totalTextLength: totalTextLength\n      };\n    } catch (error) {\n      throw new Error(`Failed to process PDF: ${error.message}`);\n    }\n  }\n  isHeaderPage(text, pageNum) {\n    const lowerText = text.toLowerCase();\n\n    // Skip pages that are clearly headers, copyright, or table of contents\n    if (pageNum <= 5) {\n      // First 5 pages are almost always headers\n      return true;\n    }\n\n    // Skip pages with common header indicators\n    if (lowerText.includes('project gutenberg') || lowerText.includes('copyright') || lowerText.includes('table of contents') || lowerText.includes('contents') || lowerText.includes('introduction') || lowerText.includes('preface') || lowerText.includes('chapter i') || lowerText.includes('book i')) {\n      return true;\n    }\n\n    // Skip very short pages (likely headers)\n    if (text.length < 200) {\n      return true;\n    }\n\n    // Skip pages that are mostly numbers or chapter titles\n    const words = text.split(/\\s+/);\n    const shortWords = words.filter(w => w.length <= 3).length;\n    if (shortWords > words.length * 0.7) {\n      return true;\n    }\n    return false;\n  }\n  splitTextIntoChunks(text, pageNum) {\n    const words = text.split(/\\s+/);\n    const chunks = [];\n\n    // Only create chunks if we have enough words\n    if (words.length < this.chunkSize) {\n      // For short pages, create one chunk\n      chunks.push({\n        text: text.trim(),\n        page: pageNum\n      });\n    } else {\n      // For longer pages, split into chunks\n      for (let i = 0; i < words.length; i += this.chunkSize - this.overlap) {\n        const chunkWords = words.slice(i, i + this.chunkSize);\n        const chunkText = chunkWords.join(' ');\n        if (chunkText.trim()) {\n          chunks.push({\n            text: chunkText,\n            page: pageNum\n          });\n        }\n      }\n    }\n    return chunks;\n  }\n\n  // Improved text search with better relevance scoring\n  searchText(textChunks, query) {\n    const queryLower = query.toLowerCase();\n    const results = [];\n    textChunks.forEach((chunk, index) => {\n      if (chunk.text.toLowerCase().includes(queryLower)) {\n        results.push({\n          chunk,\n          index,\n          relevance: this.calculateRelevance(chunk.text, query)\n        });\n      }\n    });\n\n    // Sort by relevance\n    results.sort((a, b) => b.relevance - a.relevance);\n    return results.slice(0, 5); // Return top 5 results instead of 3\n  }\n  calculateRelevance(text, query) {\n    const textLower = text.toLowerCase();\n    const queryWords = query.toLowerCase().split(/\\s+/);\n    let score = 0;\n    queryWords.forEach(word => {\n      const regex = new RegExp(word, 'gi');\n      const matches = textLower.match(regex);\n      if (matches) {\n        score += matches.length * 10; // Boost score for multiple matches\n      }\n    });\n\n    // Bonus for longer text chunks (more context)\n    score += Math.min(text.length / 100, 50);\n    return score;\n  }\n}\nexport default PDFProcessor;","map":{"version":3,"names":["pdfjsLib","GlobalWorkerOptions","workerSrc","version","PDFProcessor","constructor","chunkSize","overlap","processPDF","file","arrayBuffer","pdf","getDocument","data","promise","textChunks","totalTextLength","contentPages","skippedPages","console","log","numPages","pageNum","page","getPage","textContent","getTextContent","pageText","items","map","item","str","join","trimmedText","trim","textLength","length","isHeaderPage","chunks","splitTextIntoChunks","push","totalPages","totalChunks","error","Error","message","text","lowerText","toLowerCase","includes","words","split","shortWords","filter","w","i","chunkWords","slice","chunkText","searchText","query","queryLower","results","forEach","chunk","index","relevance","calculateRelevance","sort","a","b","textLower","queryWords","score","word","regex","RegExp","matches","match","Math","min"],"sources":["C:/Users/sanja/Pdfc/src/components/PDFProcessor.js"],"sourcesContent":["import * as pdfjsLib from 'pdfjs-dist';\r\n\r\n// Set up PDF.js worker\r\npdfjsLib.GlobalWorkerOptions.workerSrc = `//cdnjs.cloudflare.com/ajax/libs/pdf.js/${pdfjsLib.version}/pdf.worker.min.js`;\r\n\r\nclass PDFProcessor {\r\n  constructor() {\r\n    this.chunkSize = 1000; // Increased from 500 to 1000 words\r\n    this.overlap = 100;    // Increased overlap for better context\r\n  }\r\n\r\n  async processPDF(file) {\r\n    try {\r\n      // Convert file to ArrayBuffer\r\n      const arrayBuffer = await file.arrayBuffer();\r\n      \r\n      // Load PDF document\r\n      const pdf = await pdfjsLib.getDocument({ data: arrayBuffer }).promise;\r\n      \r\n      const textChunks = [];\r\n      let totalTextLength = 0;\r\n      let contentPages = 0;\r\n      let skippedPages = 0;\r\n      \r\n      console.log(`Processing PDF with ${pdf.numPages} pages...`);\r\n      \r\n      // Extract text from each page, but be smarter about skipping headers\r\n      for (let pageNum = 1; pageNum <= pdf.numPages; pageNum++) {\r\n        const page = await pdf.getPage(pageNum);\r\n        const textContent = await page.getTextContent();\r\n        \r\n        // Combine text items\r\n        const pageText = textContent.items\r\n          .map(item => item.str)\r\n          .join(' ');\r\n        \r\n        const trimmedText = pageText.trim();\r\n        const textLength = trimmedText.length;\r\n        \r\n        // Skip pages that are likely headers, copyright, or table of contents\r\n        if (this.isHeaderPage(trimmedText, pageNum)) {\r\n          console.log(`Skipping page ${pageNum} (header/toc: ${textLength} chars)`);\r\n          skippedPages++;\r\n          continue;\r\n        }\r\n        \r\n        // Only start collecting content after we've seen some substantial pages\r\n        if (contentPages < 2 && textLength < 500) {\r\n          console.log(`Skipping page ${pageNum} (too short for content: ${textLength} chars)`);\r\n          skippedPages++;\r\n          continue;\r\n        }\r\n        \r\n        totalTextLength += textLength;\r\n        contentPages++;\r\n        \r\n        console.log(`Page ${pageNum}: ${textLength} characters - ACCEPTED`);\r\n        \r\n        // Split text into chunks\r\n        const chunks = this.splitTextIntoChunks(trimmedText, pageNum);\r\n        textChunks.push(...chunks);\r\n        \r\n        // Limit to first 30 content pages to avoid overwhelming the API\r\n        if (contentPages >= 30) {\r\n          console.log(`Reached limit of 30 content pages, stopping extraction`);\r\n          break;\r\n        }\r\n      }\r\n      \r\n      console.log(`Skipped ${skippedPages} header/toc pages`);\r\n      console.log(`Extracted ${contentPages} content pages with ${textChunks.length} chunks`);\r\n      console.log(`Total text length: ${totalTextLength} characters`);\r\n      \r\n      return {\r\n        totalPages: pdf.numPages,\r\n        contentPages: contentPages,\r\n        skippedPages: skippedPages,\r\n        totalChunks: textChunks.length,\r\n        textChunks: textChunks,\r\n        totalTextLength: totalTextLength\r\n      };\r\n      \r\n    } catch (error) {\r\n      throw new Error(`Failed to process PDF: ${error.message}`);\r\n    }\r\n  }\r\n\r\n  isHeaderPage(text, pageNum) {\r\n    const lowerText = text.toLowerCase();\r\n    \r\n    // Skip pages that are clearly headers, copyright, or table of contents\r\n    if (pageNum <= 5) {\r\n      // First 5 pages are almost always headers\r\n      return true;\r\n    }\r\n    \r\n    // Skip pages with common header indicators\r\n    if (lowerText.includes('project gutenberg') || \r\n        lowerText.includes('copyright') ||\r\n        lowerText.includes('table of contents') ||\r\n        lowerText.includes('contents') ||\r\n        lowerText.includes('introduction') ||\r\n        lowerText.includes('preface') ||\r\n        lowerText.includes('chapter i') ||\r\n        lowerText.includes('book i')) {\r\n      return true;\r\n    }\r\n    \r\n    // Skip very short pages (likely headers)\r\n    if (text.length < 200) {\r\n      return true;\r\n    }\r\n    \r\n    // Skip pages that are mostly numbers or chapter titles\r\n    const words = text.split(/\\s+/);\r\n    const shortWords = words.filter(w => w.length <= 3).length;\r\n    if (shortWords > words.length * 0.7) {\r\n      return true;\r\n    }\r\n    \r\n    return false;\r\n  }\r\n\r\n  splitTextIntoChunks(text, pageNum) {\r\n    const words = text.split(/\\s+/);\r\n    const chunks = [];\r\n    \r\n    // Only create chunks if we have enough words\r\n    if (words.length < this.chunkSize) {\r\n      // For short pages, create one chunk\r\n      chunks.push({\r\n        text: text.trim(),\r\n        page: pageNum\r\n      });\r\n    } else {\r\n      // For longer pages, split into chunks\r\n      for (let i = 0; i < words.length; i += this.chunkSize - this.overlap) {\r\n        const chunkWords = words.slice(i, i + this.chunkSize);\r\n        const chunkText = chunkWords.join(' ');\r\n        \r\n        if (chunkText.trim()) {\r\n          chunks.push({\r\n            text: chunkText,\r\n            page: pageNum\r\n          });\r\n        }\r\n      }\r\n    }\r\n    \r\n    return chunks;\r\n  }\r\n\r\n  // Improved text search with better relevance scoring\r\n  searchText(textChunks, query) {\r\n    const queryLower = query.toLowerCase();\r\n    const results = [];\r\n    \r\n    textChunks.forEach((chunk, index) => {\r\n      if (chunk.text.toLowerCase().includes(queryLower)) {\r\n        results.push({\r\n          chunk,\r\n          index,\r\n          relevance: this.calculateRelevance(chunk.text, query)\r\n        });\r\n      }\r\n    });\r\n    \r\n    // Sort by relevance\r\n    results.sort((a, b) => b.relevance - a.relevance);\r\n    \r\n    return results.slice(0, 5); // Return top 5 results instead of 3\r\n  }\r\n\r\n  calculateRelevance(text, query) {\r\n    const textLower = text.toLowerCase();\r\n    const queryWords = query.toLowerCase().split(/\\s+/);\r\n    \r\n    let score = 0;\r\n    queryWords.forEach(word => {\r\n      const regex = new RegExp(word, 'gi');\r\n      const matches = textLower.match(regex);\r\n      if (matches) {\r\n        score += matches.length * 10; // Boost score for multiple matches\r\n      }\r\n    });\r\n    \r\n    // Bonus for longer text chunks (more context)\r\n    score += Math.min(text.length / 100, 50);\r\n    \r\n    return score;\r\n  }\r\n}\r\n\r\nexport default PDFProcessor;\r\n"],"mappings":"AAAA,OAAO,KAAKA,QAAQ,MAAM,YAAY;;AAEtC;AACAA,QAAQ,CAACC,mBAAmB,CAACC,SAAS,GAAG,2CAA2CF,QAAQ,CAACG,OAAO,oBAAoB;AAExH,MAAMC,YAAY,CAAC;EACjBC,WAAWA,CAAA,EAAG;IACZ,IAAI,CAACC,SAAS,GAAG,IAAI,CAAC,CAAC;IACvB,IAAI,CAACC,OAAO,GAAG,GAAG,CAAC,CAAI;EACzB;EAEA,MAAMC,UAAUA,CAACC,IAAI,EAAE;IACrB,IAAI;MACF;MACA,MAAMC,WAAW,GAAG,MAAMD,IAAI,CAACC,WAAW,CAAC,CAAC;;MAE5C;MACA,MAAMC,GAAG,GAAG,MAAMX,QAAQ,CAACY,WAAW,CAAC;QAAEC,IAAI,EAAEH;MAAY,CAAC,CAAC,CAACI,OAAO;MAErE,MAAMC,UAAU,GAAG,EAAE;MACrB,IAAIC,eAAe,GAAG,CAAC;MACvB,IAAIC,YAAY,GAAG,CAAC;MACpB,IAAIC,YAAY,GAAG,CAAC;MAEpBC,OAAO,CAACC,GAAG,CAAC,uBAAuBT,GAAG,CAACU,QAAQ,WAAW,CAAC;;MAE3D;MACA,KAAK,IAAIC,OAAO,GAAG,CAAC,EAAEA,OAAO,IAAIX,GAAG,CAACU,QAAQ,EAAEC,OAAO,EAAE,EAAE;QACxD,MAAMC,IAAI,GAAG,MAAMZ,GAAG,CAACa,OAAO,CAACF,OAAO,CAAC;QACvC,MAAMG,WAAW,GAAG,MAAMF,IAAI,CAACG,cAAc,CAAC,CAAC;;QAE/C;QACA,MAAMC,QAAQ,GAAGF,WAAW,CAACG,KAAK,CAC/BC,GAAG,CAACC,IAAI,IAAIA,IAAI,CAACC,GAAG,CAAC,CACrBC,IAAI,CAAC,GAAG,CAAC;QAEZ,MAAMC,WAAW,GAAGN,QAAQ,CAACO,IAAI,CAAC,CAAC;QACnC,MAAMC,UAAU,GAAGF,WAAW,CAACG,MAAM;;QAErC;QACA,IAAI,IAAI,CAACC,YAAY,CAACJ,WAAW,EAAEX,OAAO,CAAC,EAAE;UAC3CH,OAAO,CAACC,GAAG,CAAC,iBAAiBE,OAAO,iBAAiBa,UAAU,SAAS,CAAC;UACzEjB,YAAY,EAAE;UACd;QACF;;QAEA;QACA,IAAID,YAAY,GAAG,CAAC,IAAIkB,UAAU,GAAG,GAAG,EAAE;UACxChB,OAAO,CAACC,GAAG,CAAC,iBAAiBE,OAAO,4BAA4Ba,UAAU,SAAS,CAAC;UACpFjB,YAAY,EAAE;UACd;QACF;QAEAF,eAAe,IAAImB,UAAU;QAC7BlB,YAAY,EAAE;QAEdE,OAAO,CAACC,GAAG,CAAC,QAAQE,OAAO,KAAKa,UAAU,wBAAwB,CAAC;;QAEnE;QACA,MAAMG,MAAM,GAAG,IAAI,CAACC,mBAAmB,CAACN,WAAW,EAAEX,OAAO,CAAC;QAC7DP,UAAU,CAACyB,IAAI,CAAC,GAAGF,MAAM,CAAC;;QAE1B;QACA,IAAIrB,YAAY,IAAI,EAAE,EAAE;UACtBE,OAAO,CAACC,GAAG,CAAC,wDAAwD,CAAC;UACrE;QACF;MACF;MAEAD,OAAO,CAACC,GAAG,CAAC,WAAWF,YAAY,mBAAmB,CAAC;MACvDC,OAAO,CAACC,GAAG,CAAC,aAAaH,YAAY,uBAAuBF,UAAU,CAACqB,MAAM,SAAS,CAAC;MACvFjB,OAAO,CAACC,GAAG,CAAC,sBAAsBJ,eAAe,aAAa,CAAC;MAE/D,OAAO;QACLyB,UAAU,EAAE9B,GAAG,CAACU,QAAQ;QACxBJ,YAAY,EAAEA,YAAY;QAC1BC,YAAY,EAAEA,YAAY;QAC1BwB,WAAW,EAAE3B,UAAU,CAACqB,MAAM;QAC9BrB,UAAU,EAAEA,UAAU;QACtBC,eAAe,EAAEA;MACnB,CAAC;IAEH,CAAC,CAAC,OAAO2B,KAAK,EAAE;MACd,MAAM,IAAIC,KAAK,CAAC,0BAA0BD,KAAK,CAACE,OAAO,EAAE,CAAC;IAC5D;EACF;EAEAR,YAAYA,CAACS,IAAI,EAAExB,OAAO,EAAE;IAC1B,MAAMyB,SAAS,GAAGD,IAAI,CAACE,WAAW,CAAC,CAAC;;IAEpC;IACA,IAAI1B,OAAO,IAAI,CAAC,EAAE;MAChB;MACA,OAAO,IAAI;IACb;;IAEA;IACA,IAAIyB,SAAS,CAACE,QAAQ,CAAC,mBAAmB,CAAC,IACvCF,SAAS,CAACE,QAAQ,CAAC,WAAW,CAAC,IAC/BF,SAAS,CAACE,QAAQ,CAAC,mBAAmB,CAAC,IACvCF,SAAS,CAACE,QAAQ,CAAC,UAAU,CAAC,IAC9BF,SAAS,CAACE,QAAQ,CAAC,cAAc,CAAC,IAClCF,SAAS,CAACE,QAAQ,CAAC,SAAS,CAAC,IAC7BF,SAAS,CAACE,QAAQ,CAAC,WAAW,CAAC,IAC/BF,SAAS,CAACE,QAAQ,CAAC,QAAQ,CAAC,EAAE;MAChC,OAAO,IAAI;IACb;;IAEA;IACA,IAAIH,IAAI,CAACV,MAAM,GAAG,GAAG,EAAE;MACrB,OAAO,IAAI;IACb;;IAEA;IACA,MAAMc,KAAK,GAAGJ,IAAI,CAACK,KAAK,CAAC,KAAK,CAAC;IAC/B,MAAMC,UAAU,GAAGF,KAAK,CAACG,MAAM,CAACC,CAAC,IAAIA,CAAC,CAAClB,MAAM,IAAI,CAAC,CAAC,CAACA,MAAM;IAC1D,IAAIgB,UAAU,GAAGF,KAAK,CAACd,MAAM,GAAG,GAAG,EAAE;MACnC,OAAO,IAAI;IACb;IAEA,OAAO,KAAK;EACd;EAEAG,mBAAmBA,CAACO,IAAI,EAAExB,OAAO,EAAE;IACjC,MAAM4B,KAAK,GAAGJ,IAAI,CAACK,KAAK,CAAC,KAAK,CAAC;IAC/B,MAAMb,MAAM,GAAG,EAAE;;IAEjB;IACA,IAAIY,KAAK,CAACd,MAAM,GAAG,IAAI,CAAC9B,SAAS,EAAE;MACjC;MACAgC,MAAM,CAACE,IAAI,CAAC;QACVM,IAAI,EAAEA,IAAI,CAACZ,IAAI,CAAC,CAAC;QACjBX,IAAI,EAAED;MACR,CAAC,CAAC;IACJ,CAAC,MAAM;MACL;MACA,KAAK,IAAIiC,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGL,KAAK,CAACd,MAAM,EAAEmB,CAAC,IAAI,IAAI,CAACjD,SAAS,GAAG,IAAI,CAACC,OAAO,EAAE;QACpE,MAAMiD,UAAU,GAAGN,KAAK,CAACO,KAAK,CAACF,CAAC,EAAEA,CAAC,GAAG,IAAI,CAACjD,SAAS,CAAC;QACrD,MAAMoD,SAAS,GAAGF,UAAU,CAACxB,IAAI,CAAC,GAAG,CAAC;QAEtC,IAAI0B,SAAS,CAACxB,IAAI,CAAC,CAAC,EAAE;UACpBI,MAAM,CAACE,IAAI,CAAC;YACVM,IAAI,EAAEY,SAAS;YACfnC,IAAI,EAAED;UACR,CAAC,CAAC;QACJ;MACF;IACF;IAEA,OAAOgB,MAAM;EACf;;EAEA;EACAqB,UAAUA,CAAC5C,UAAU,EAAE6C,KAAK,EAAE;IAC5B,MAAMC,UAAU,GAAGD,KAAK,CAACZ,WAAW,CAAC,CAAC;IACtC,MAAMc,OAAO,GAAG,EAAE;IAElB/C,UAAU,CAACgD,OAAO,CAAC,CAACC,KAAK,EAAEC,KAAK,KAAK;MACnC,IAAID,KAAK,CAAClB,IAAI,CAACE,WAAW,CAAC,CAAC,CAACC,QAAQ,CAACY,UAAU,CAAC,EAAE;QACjDC,OAAO,CAACtB,IAAI,CAAC;UACXwB,KAAK;UACLC,KAAK;UACLC,SAAS,EAAE,IAAI,CAACC,kBAAkB,CAACH,KAAK,CAAClB,IAAI,EAAEc,KAAK;QACtD,CAAC,CAAC;MACJ;IACF,CAAC,CAAC;;IAEF;IACAE,OAAO,CAACM,IAAI,CAAC,CAACC,CAAC,EAAEC,CAAC,KAAKA,CAAC,CAACJ,SAAS,GAAGG,CAAC,CAACH,SAAS,CAAC;IAEjD,OAAOJ,OAAO,CAACL,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;EAC9B;EAEAU,kBAAkBA,CAACrB,IAAI,EAAEc,KAAK,EAAE;IAC9B,MAAMW,SAAS,GAAGzB,IAAI,CAACE,WAAW,CAAC,CAAC;IACpC,MAAMwB,UAAU,GAAGZ,KAAK,CAACZ,WAAW,CAAC,CAAC,CAACG,KAAK,CAAC,KAAK,CAAC;IAEnD,IAAIsB,KAAK,GAAG,CAAC;IACbD,UAAU,CAACT,OAAO,CAACW,IAAI,IAAI;MACzB,MAAMC,KAAK,GAAG,IAAIC,MAAM,CAACF,IAAI,EAAE,IAAI,CAAC;MACpC,MAAMG,OAAO,GAAGN,SAAS,CAACO,KAAK,CAACH,KAAK,CAAC;MACtC,IAAIE,OAAO,EAAE;QACXJ,KAAK,IAAII,OAAO,CAACzC,MAAM,GAAG,EAAE,CAAC,CAAC;MAChC;IACF,CAAC,CAAC;;IAEF;IACAqC,KAAK,IAAIM,IAAI,CAACC,GAAG,CAAClC,IAAI,CAACV,MAAM,GAAG,GAAG,EAAE,EAAE,CAAC;IAExC,OAAOqC,KAAK;EACd;AACF;AAEA,eAAerE,YAAY","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}