{"ast":null,"code":"import * as pdfjsLib from 'pdfjs-dist';\n\n// Set up PDF.js worker\npdfjsLib.GlobalWorkerOptions.workerSrc = `//cdnjs.cloudflare.com/ajax/libs/pdf.js/${pdfjsLib.version}/pdf.worker.min.js`;\nclass PDFProcessor {\n  constructor() {\n    this.chunkSize = 500;\n    this.overlap = 50;\n  }\n  async processPDF(file) {\n    try {\n      // Convert file to ArrayBuffer\n      const arrayBuffer = await file.arrayBuffer();\n\n      // Load PDF document\n      const pdf = await pdfjsLib.getDocument({\n        data: arrayBuffer\n      }).promise;\n      const textChunks = [];\n      let totalTextLength = 0;\n\n      // Extract text from each page\n      for (let pageNum = 1; pageNum <= pdf.numPages; pageNum++) {\n        const page = await pdf.getPage(pageNum);\n        const textContent = await page.getTextContent();\n\n        // Combine text items\n        const pageText = textContent.items.map(item => item.str).join(' ');\n        totalTextLength += pageText.length;\n\n        // Split text into chunks\n        const chunks = this.splitTextIntoChunks(pageText, pageNum);\n        textChunks.push(...chunks);\n      }\n      return {\n        totalPages: pdf.numPages,\n        totalChunks: textChunks.length,\n        textChunks: textChunks,\n        totalTextLength: totalTextLength\n      };\n    } catch (error) {\n      throw new Error(`Failed to process PDF: ${error.message}`);\n    }\n  }\n  splitTextIntoChunks(text, pageNum) {\n    const words = text.split(/\\s+/);\n    const chunks = [];\n    for (let i = 0; i < words.length; i += this.chunkSize - this.overlap) {\n      const chunkWords = words.slice(i, i + this.chunkSize);\n      const chunkText = chunkWords.join(' ');\n      if (chunkText.trim()) {\n        chunks.push({\n          text: chunkText,\n          page: pageNum\n        });\n      }\n    }\n    return chunks;\n  }\n\n  // Simple text search (for demo purposes - in production you'd use embeddings)\n  searchText(textChunks, query) {\n    const queryLower = query.toLowerCase();\n    const results = [];\n    textChunks.forEach((chunk, index) => {\n      if (chunk.text.toLowerCase().includes(queryLower)) {\n        results.push({\n          chunk,\n          index,\n          relevance: this.calculateRelevance(chunk.text, query)\n        });\n      }\n    });\n\n    // Sort by relevance\n    results.sort((a, b) => b.relevance - a.relevance);\n    return results.slice(0, 3); // Return top 3 results\n  }\n  calculateRelevance(text, query) {\n    const textLower = text.toLowerCase();\n    const queryWords = query.toLowerCase().split(/\\s+/);\n    let score = 0;\n    queryWords.forEach(word => {\n      const regex = new RegExp(word, 'gi');\n      const matches = textLower.match(regex);\n      if (matches) {\n        score += matches.length;\n      }\n    });\n    return score;\n  }\n}\nexport default PDFProcessor;","map":{"version":3,"names":["pdfjsLib","GlobalWorkerOptions","workerSrc","version","PDFProcessor","constructor","chunkSize","overlap","processPDF","file","arrayBuffer","pdf","getDocument","data","promise","textChunks","totalTextLength","pageNum","numPages","page","getPage","textContent","getTextContent","pageText","items","map","item","str","join","length","chunks","splitTextIntoChunks","push","totalPages","totalChunks","error","Error","message","text","words","split","i","chunkWords","slice","chunkText","trim","searchText","query","queryLower","toLowerCase","results","forEach","chunk","index","includes","relevance","calculateRelevance","sort","a","b","textLower","queryWords","score","word","regex","RegExp","matches","match"],"sources":["C:/Users/sanja/Pdfc/src/components/PDFProcessor.js"],"sourcesContent":["import * as pdfjsLib from 'pdfjs-dist';\r\n\r\n// Set up PDF.js worker\r\npdfjsLib.GlobalWorkerOptions.workerSrc = `//cdnjs.cloudflare.com/ajax/libs/pdf.js/${pdfjsLib.version}/pdf.worker.min.js`;\r\n\r\nclass PDFProcessor {\r\n  constructor() {\r\n    this.chunkSize = 500;\r\n    this.overlap = 50;\r\n  }\r\n\r\n  async processPDF(file) {\r\n    try {\r\n      // Convert file to ArrayBuffer\r\n      const arrayBuffer = await file.arrayBuffer();\r\n      \r\n      // Load PDF document\r\n      const pdf = await pdfjsLib.getDocument({ data: arrayBuffer }).promise;\r\n      \r\n      const textChunks = [];\r\n      let totalTextLength = 0;\r\n      \r\n      // Extract text from each page\r\n      for (let pageNum = 1; pageNum <= pdf.numPages; pageNum++) {\r\n        const page = await pdf.getPage(pageNum);\r\n        const textContent = await page.getTextContent();\r\n        \r\n        // Combine text items\r\n        const pageText = textContent.items\r\n          .map(item => item.str)\r\n          .join(' ');\r\n        \r\n        totalTextLength += pageText.length;\r\n        \r\n        // Split text into chunks\r\n        const chunks = this.splitTextIntoChunks(pageText, pageNum);\r\n        textChunks.push(...chunks);\r\n      }\r\n      \r\n      return {\r\n        totalPages: pdf.numPages,\r\n        totalChunks: textChunks.length,\r\n        textChunks: textChunks,\r\n        totalTextLength: totalTextLength\r\n      };\r\n      \r\n    } catch (error) {\r\n      throw new Error(`Failed to process PDF: ${error.message}`);\r\n    }\r\n  }\r\n\r\n  splitTextIntoChunks(text, pageNum) {\r\n    const words = text.split(/\\s+/);\r\n    const chunks = [];\r\n    \r\n    for (let i = 0; i < words.length; i += this.chunkSize - this.overlap) {\r\n      const chunkWords = words.slice(i, i + this.chunkSize);\r\n      const chunkText = chunkWords.join(' ');\r\n      \r\n      if (chunkText.trim()) {\r\n        chunks.push({\r\n          text: chunkText,\r\n          page: pageNum\r\n        });\r\n      }\r\n    }\r\n    \r\n    return chunks;\r\n  }\r\n\r\n  // Simple text search (for demo purposes - in production you'd use embeddings)\r\n  searchText(textChunks, query) {\r\n    const queryLower = query.toLowerCase();\r\n    const results = [];\r\n    \r\n    textChunks.forEach((chunk, index) => {\r\n      if (chunk.text.toLowerCase().includes(queryLower)) {\r\n        results.push({\r\n          chunk,\r\n          index,\r\n          relevance: this.calculateRelevance(chunk.text, query)\r\n        });\r\n      }\r\n    });\r\n    \r\n    // Sort by relevance\r\n    results.sort((a, b) => b.relevance - a.relevance);\r\n    \r\n    return results.slice(0, 3); // Return top 3 results\r\n  }\r\n\r\n  calculateRelevance(text, query) {\r\n    const textLower = text.toLowerCase();\r\n    const queryWords = query.toLowerCase().split(/\\s+/);\r\n    \r\n    let score = 0;\r\n    queryWords.forEach(word => {\r\n      const regex = new RegExp(word, 'gi');\r\n      const matches = textLower.match(regex);\r\n      if (matches) {\r\n        score += matches.length;\r\n      }\r\n    });\r\n    \r\n    return score;\r\n  }\r\n}\r\n\r\nexport default PDFProcessor;\r\n"],"mappings":"AAAA,OAAO,KAAKA,QAAQ,MAAM,YAAY;;AAEtC;AACAA,QAAQ,CAACC,mBAAmB,CAACC,SAAS,GAAG,2CAA2CF,QAAQ,CAACG,OAAO,oBAAoB;AAExH,MAAMC,YAAY,CAAC;EACjBC,WAAWA,CAAA,EAAG;IACZ,IAAI,CAACC,SAAS,GAAG,GAAG;IACpB,IAAI,CAACC,OAAO,GAAG,EAAE;EACnB;EAEA,MAAMC,UAAUA,CAACC,IAAI,EAAE;IACrB,IAAI;MACF;MACA,MAAMC,WAAW,GAAG,MAAMD,IAAI,CAACC,WAAW,CAAC,CAAC;;MAE5C;MACA,MAAMC,GAAG,GAAG,MAAMX,QAAQ,CAACY,WAAW,CAAC;QAAEC,IAAI,EAAEH;MAAY,CAAC,CAAC,CAACI,OAAO;MAErE,MAAMC,UAAU,GAAG,EAAE;MACrB,IAAIC,eAAe,GAAG,CAAC;;MAEvB;MACA,KAAK,IAAIC,OAAO,GAAG,CAAC,EAAEA,OAAO,IAAIN,GAAG,CAACO,QAAQ,EAAED,OAAO,EAAE,EAAE;QACxD,MAAME,IAAI,GAAG,MAAMR,GAAG,CAACS,OAAO,CAACH,OAAO,CAAC;QACvC,MAAMI,WAAW,GAAG,MAAMF,IAAI,CAACG,cAAc,CAAC,CAAC;;QAE/C;QACA,MAAMC,QAAQ,GAAGF,WAAW,CAACG,KAAK,CAC/BC,GAAG,CAACC,IAAI,IAAIA,IAAI,CAACC,GAAG,CAAC,CACrBC,IAAI,CAAC,GAAG,CAAC;QAEZZ,eAAe,IAAIO,QAAQ,CAACM,MAAM;;QAElC;QACA,MAAMC,MAAM,GAAG,IAAI,CAACC,mBAAmB,CAACR,QAAQ,EAAEN,OAAO,CAAC;QAC1DF,UAAU,CAACiB,IAAI,CAAC,GAAGF,MAAM,CAAC;MAC5B;MAEA,OAAO;QACLG,UAAU,EAAEtB,GAAG,CAACO,QAAQ;QACxBgB,WAAW,EAAEnB,UAAU,CAACc,MAAM;QAC9Bd,UAAU,EAAEA,UAAU;QACtBC,eAAe,EAAEA;MACnB,CAAC;IAEH,CAAC,CAAC,OAAOmB,KAAK,EAAE;MACd,MAAM,IAAIC,KAAK,CAAC,0BAA0BD,KAAK,CAACE,OAAO,EAAE,CAAC;IAC5D;EACF;EAEAN,mBAAmBA,CAACO,IAAI,EAAErB,OAAO,EAAE;IACjC,MAAMsB,KAAK,GAAGD,IAAI,CAACE,KAAK,CAAC,KAAK,CAAC;IAC/B,MAAMV,MAAM,GAAG,EAAE;IAEjB,KAAK,IAAIW,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGF,KAAK,CAACV,MAAM,EAAEY,CAAC,IAAI,IAAI,CAACnC,SAAS,GAAG,IAAI,CAACC,OAAO,EAAE;MACpE,MAAMmC,UAAU,GAAGH,KAAK,CAACI,KAAK,CAACF,CAAC,EAAEA,CAAC,GAAG,IAAI,CAACnC,SAAS,CAAC;MACrD,MAAMsC,SAAS,GAAGF,UAAU,CAACd,IAAI,CAAC,GAAG,CAAC;MAEtC,IAAIgB,SAAS,CAACC,IAAI,CAAC,CAAC,EAAE;QACpBf,MAAM,CAACE,IAAI,CAAC;UACVM,IAAI,EAAEM,SAAS;UACfzB,IAAI,EAAEF;QACR,CAAC,CAAC;MACJ;IACF;IAEA,OAAOa,MAAM;EACf;;EAEA;EACAgB,UAAUA,CAAC/B,UAAU,EAAEgC,KAAK,EAAE;IAC5B,MAAMC,UAAU,GAAGD,KAAK,CAACE,WAAW,CAAC,CAAC;IACtC,MAAMC,OAAO,GAAG,EAAE;IAElBnC,UAAU,CAACoC,OAAO,CAAC,CAACC,KAAK,EAAEC,KAAK,KAAK;MACnC,IAAID,KAAK,CAACd,IAAI,CAACW,WAAW,CAAC,CAAC,CAACK,QAAQ,CAACN,UAAU,CAAC,EAAE;QACjDE,OAAO,CAAClB,IAAI,CAAC;UACXoB,KAAK;UACLC,KAAK;UACLE,SAAS,EAAE,IAAI,CAACC,kBAAkB,CAACJ,KAAK,CAACd,IAAI,EAAES,KAAK;QACtD,CAAC,CAAC;MACJ;IACF,CAAC,CAAC;;IAEF;IACAG,OAAO,CAACO,IAAI,CAAC,CAACC,CAAC,EAAEC,CAAC,KAAKA,CAAC,CAACJ,SAAS,GAAGG,CAAC,CAACH,SAAS,CAAC;IAEjD,OAAOL,OAAO,CAACP,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;EAC9B;EAEAa,kBAAkBA,CAAClB,IAAI,EAAES,KAAK,EAAE;IAC9B,MAAMa,SAAS,GAAGtB,IAAI,CAACW,WAAW,CAAC,CAAC;IACpC,MAAMY,UAAU,GAAGd,KAAK,CAACE,WAAW,CAAC,CAAC,CAACT,KAAK,CAAC,KAAK,CAAC;IAEnD,IAAIsB,KAAK,GAAG,CAAC;IACbD,UAAU,CAACV,OAAO,CAACY,IAAI,IAAI;MACzB,MAAMC,KAAK,GAAG,IAAIC,MAAM,CAACF,IAAI,EAAE,IAAI,CAAC;MACpC,MAAMG,OAAO,GAAGN,SAAS,CAACO,KAAK,CAACH,KAAK,CAAC;MACtC,IAAIE,OAAO,EAAE;QACXJ,KAAK,IAAII,OAAO,CAACrC,MAAM;MACzB;IACF,CAAC,CAAC;IAEF,OAAOiC,KAAK;EACd;AACF;AAEA,eAAe1D,YAAY","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}